# Feed Forward Neural Network
Author: Samantha E. Reksosamudra

## Abstract
The goal of this project is to train a feed forward neural network with two types of dataset: a given array in class and the MNIST dataset. Then we test the neural network and compare the least square errors of two training/testing approaces for the given array dataset. The results correspond to a previous exercise (HW 1 - Curve Fit Training Models) where the least squares error is significantly smaller when we train the model with the first and last 10 datapoints (LSE: 20.62) compared with the first 20 datapoints (LSE: 1666.60). We also compared the accuracies of the neural network against the LSTM (Long Short Term Memory), SVM (Support Vector Machine), and Decision Tree classifiers. 
 
## Sec. I. Introduction and Overview
Feed forward neural networks are a popular type of neural network for solving machine learning tasks, such as classification, regression, and prediction. In this project, we will train a feed forward neural network with a given array and the MNIST dataset. The goal is to optimize the network's parameters and architecture to achieve the smallest error and highest possible accuracy on a test set. 

This project consists of: (1) data preparation by preprocessing, splitting into training and test datasets, and converting them into a format compatible with the neural network, (2) design the model architecture of the feed forward nueral network, (3) train the network using a training algorithm such as backpropagation, (4) evaluate the performance on a test set and generate the error and/or accuracy computation, (5) adjust the parameters, such as learning rate and batch size, by hyperparameter tuning.

## Sec. II. Theoretical Background
The MNIST dataset is a large database containing 70,000 images (28 x 28) of handwritten digits from 0 to 9. It is often used for traning and testing in image processing and machine learning. Based on each image's labels, we can train and classify each image to predict the a given image's label (or also called as cluster). 

Feed forward neural networks are designed to process input data by propagating it through multiple layers of interconnected neurons, which apply nonlinear transformations to the data and extract useful features. The output of the network is generated by the final layer, which produces the prediction or classification result.

Using KMeans (another clustering method), these are the clusters for each digits in the MNIST dataset, where each cluster represents a digit. 

![](mnist_clusters.png)

LSTM is a type of recurrent neural network (RNN) architecture that is widely used in machine learning due to its capabilities for capturing long-term dependencies in sequential data. It overcomes the vanisihing gradient problem which happens during backpropagation by using a special memory cell that can selectively remember or forget previous inputs. Some LTSM applications include speech recognition, natural language processing, and time series prediction.

We could use LTSM, along with SVM and decision tree classifiers to classify and separate the dataset based on their labels. In this project, we will compare the accuracies of these different classifier methods.

## Sec. III Algorithm Implementation
  ### Load the Datasets
  Firstly, we load the MNIST dataset using ```datasets.MNIST``` imported from ```torchvision``` and the given array in class:
  
  ```
  import torchvision.datasets as datasets
  
# Load the data
X = np.arange(0,31).reshape(-1,1)
y = np.array([30,35,33,32,34,37,39,38,36,36,37,39,42,45,45,41,40,39,42,44,47,49,50,49,46,48,50,53,55,54,53])

# Load the MNIST dataset and apply transformations
train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())
  ```
  
  ### Design the Neural Network Architecture
  Using hyperparameter tuning, we created a 3-layer feed forward neural network architecture:
  
  ```
  # Define the neural network architecture 
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(1, 15)
        self.fc2 = nn.Linear(15, 6)
        self.fc3 = nn.Linear(6, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x
  ```
  
 ### Initialize Network and Define Loss Function and Optimizer
 We use Mean Squared Error (MSE) as the loss function to compute the given dataset in class and Cross Entropy Loss for the MNIST dataset.
 ```
 # Initialize the network and define the loss function and optimizer
  net = Net()
  criterion = nn.MSELoss()  # Mean Squared Error
  optimizer = torch.optim.SGD(net.parameters(), lr=0.01)
 ```
 ```
 # loss function for MNIST dataset
  criterion = nn.CrossEntropyLoss()
 ```
 
 ### Split the Dataset into Training and Test Sets
 We will split the first dataset (the given array in class) into two groups of training and test sets, which we will compare the loss function computation against each other.
 ```
 # Split the data into training (first 20) and test data 
X_train = X[:20].astype(np.float32)
y_train = y[:20].astype(np.float32)
X_test = X[20:].astype(np.float32)
y_test = y[20:].astype(np.float32)

# Split the data into training (10 first and 10 last) and test data
X_train2 = np.concatenate((X[:10], X[21:])).astype(np.float32)
y_train2 = np.concatenate((y[:10], y[21:])).astype(np.float32)
X_test2 = X[10:21].astype(np.float32)
y_test2 = y[10:21].astype(np.float32)
 ```
 
 ### Perform Tensor Transformation
 Below is an example from the first training/test set of the given dataset in class, of how to convert numpy arrays to tensor format (which compatible to be inputted into the neural network):
```
# Convert the datasets into tensor form
X_train_tensor = torch.from_numpy(X_train)
y_train_tensor = torch.from_numpy(y_train)
X_test_tensor = torch.from_numpy(X_test)
y_test_tensor = torch.from_numpy(y_test)
```


 ### Train the Network and Generate Test Results
 Then we set the number of epochs we want, and run the training set into the algorithm. After it is done, it will evaluate its performance on the test set and compute the least squares error of the dataset.
 ```
# Train the network
num_epochs = 100
for epoch in range(num_epochs):
    optimizer.zero_grad()    # zero the gradients
    outputs = net(X_train_tensor)    # forward pass 
    loss = criterion(outputs, y_train_tensor)   # compute the loss
    loss.backward()   # backward pass
    optimizer.step()   # update the weights
        
    
    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

# Test the network
with torch.no_grad():
    outputs = net(X_test_tensor)
    compute_mse = criterion(outputs, y_test_tensor)
        
    print('Least squares error of 10 test data: {}'.format(compute_mse.item()))
 ```
 ### Preprocessing the MNIST Dataset

```
# Load the MNIST dataset and apply transformations
train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())

# Get the pixel values for all images in the dataset
train_data = train_dataset.data.float()  # Converet to float data type
train_labels = train_dataset.targets
test_data = test_dataset.data.float()
test_labels = test_dataset.targets
```

### Perform PCA on the Digit Images
After we reshape the matrices of the MNIST datapoints into a 1D vector, we compute the first 20 PCA modes on the digit images:

```
# Reshape the 2D images into 1D vectors
X_train = X_train.view(X_train.size(0), -1)
X_test = X_test.view(X_test.size(0), -1)

# Perform PCA on the first 20 modes
pca = PCA(n_components=20)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

```

### Perform Tensor Transformation and Create Data Loaders

```
# Convert numpy arrays back to tensors
X_train_pca_tensor = torch.tensor(X_train_pca, dtype=torch.float32)
X_test_pca_tensor = torch.tensor(X_test_pca, dtype=torch.float32)

# Create dataloaders for the training and test data
train_data = torch.utils.data.TensorDataset(X_train_pca_tensor, y_train)
test_data = torch.utils.data.TensorDataset(X_test_pca_tensor, y_test)

# Create data loaders
train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)

```

  ### Train the Network and Generate the Accuracy of Test Results
  The code below will activate the neural network architecture, loss function, and optimizer. Then it will train the network and compute the accuracy results of the neural network:
```
net = Net()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), lr=0.01)

# Train the network
num_epochs = 10
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = net(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        if (i+1) % 100 == 0:
            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))

# Test the network
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_loader:
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))
```

### Generate Accuracy Test Results using LSTM
Firstly, we design the LSTM network architecture. Then we split the dataset into training and test sets, before training it using LSTM and evaluate it on the test set:
```
# Define the LSTM network architecture
class LSTMNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTMNet, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)

        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Hyperparameters
input_size = 28
sequence_length = 28
hidden_size = 128
num_layers = 2
num_classes = 10
batch_size = 64
num_epochs = 5
learning_rate = 0.01

# Load the MNIST dataset
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = LSTMNet(input_size, hidden_size, num_layers, num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Train the model
total_step = len(train_loader)
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        images = images.reshape(-1, sequence_length, input_size).to(device)
        labels = labels.to(device)

        outputs = model(images)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (i + 1) % 100 == 0:
            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))

# Test the model
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_loader:
        images = images.reshape(-1, sequence_length, input_size).to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))
```
### Generate Accuracy Test Results using SVM
Load the MNIST dataset using ```fetch_openml``` to load it as a numpy array and preprocess the dataset using PCA and split it into training and test sets:

```
# Load the MNIST dataset from OpenML
mnist = fetch_openml('mnist_784')

# Convert the data and labels to numpy arrays
X = mnist.data.astype('float32') / 255.0    
y = mnist.target.astype('int32')

# Perform PCA to reduce the dimensionality of the data
pca = PCA(n_components=20)
X_pca_train = pca.fit_transform(X)

# Split the whole dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_pca_train, y, test_size=0.2, random_state=42)
```
Then we use SVM to classify and train the data, before we evaluate it on the test set:

```
# Train an SVM classifier on the training set
clf = svm.SVC(kernel='linear', C=1)
clf.fit(X_train, y_train)

# Evaluate the classifier on the test set
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
```
### Generate Accuracy Test Results using Decision Tree Classifiers
The algorithm is similar to SVM, but we would use decision tree as the classifier method this time:
```
# Train a decision tree classifier on the training set
clf = DecisionTreeClassifier(max_depth=10)
clf.fit(X_train, y_train)

# Evaluate the classifier on the test set
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
```
  
## Sec. IV. Computational Results
  ### 
 
 ### Comparison Between SVM, LDA, and Decision Tree Classifiers
 After using a classifier method on the training dataset and predict the test datasets for all the digits, the accuracy score was computed for SVM, LDA, and Decision Tree Classifiers. It is found that SVM, LDA, and Decision Tree Classifiers have an accuracy score of 83.8%, 79.5%, and 77.0% respectively. Although it took the longest time to compute, the most accurate classifier method is SVM.
 
 

## Sec. V. Summary and Conclusions
The SVD analysis is an effective method to find the most important features that structures a dataset. We found there are about 100 modes that are significantly important in the MNIST dataset. And we also tried different classifier methods on the dataset, where we found SVM to be the best predictor with the highest accuracy score compared with LDA and Decision Tree Classifiers. It is also interesting to see the digit pairs that are the hardest (4 and 9) and easiest (0 and 1) to separate in the dataset using classifier methods. 

